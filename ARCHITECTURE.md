# Architecture

This document describes the architecture of the OCLite Copilot Extension.

## System Overview

OCLite is built on a **cloud-native, serverless architecture** that separates concerns into three layers: the VS Code client, the intelligence layer (LLM), and the generation layer (image models).

```
┌─────────────────────────────────────────────────────────────┐
│  VS Code Extension Host                                     │
│                                                             │
│  ┌────────────────┐     ┌─────────────────────────────────┐ │
│  │ Chat (@oclite)  │     │ Sidebar (Webview UI Toolkit)    │ │
│  └───────┬────────┘     └──────────────┬──────────────────┘ │
│          │                             │                    │
│          └──────────┬──────────────────┘                    │
│                     ▼                                       │
│            ┌────────────────┐                               │
│            │   AIService    │  ← Workspace Scanner          │
│            │   (ai.ts)      │  ← Context-Aware Suggestions  │
│            └───────┬────────┘                               │
│                    │                                        │
└────────────────────┼────────────────────────────────────────┘
                     │
        ┌────────────┴────────────┐
        ▼                         ▼
┌────────────────────┐   ┌────────────────────┐
│  Intelligence       │   │  Image Generation   │
│  SumoPod AI         │   │  OCLite API         │
│  (GPT-4o mini)     │   │  (SDXL, Flux …)     │
│                    │   │                     │
│  /v1/chat/         │   │  /api/v1/generate   │
│  completions       │   │  /api/predictions   │
└────────────────────┘   └────────────────────┘
        │                         │
        ▼                         ▼
┌────────────────────┐   ┌────────────────────┐
│  Azure Key Vault    │   │  Replicate          │
│  (Secrets Mgmt)     │   │  (GPU Inference)    │
└────────────────────┘   └────────────────────┘
```

## Cloud-Native Infrastructure

### Serverless Orchestration

API processing is handled through **Azure Functions (Node.js)**, providing automatic scaling and pay-as-you-go cost efficiency. The extension never communicates directly with GPU infrastructure — all requests are routed through the serverless layer.

### Image CDN — ImageKit

Generated images are uploaded to **ImageKit** as a public CDN. The workflow:

1. Raw PNG buffer is uploaded to ImageKit via the `@imagekit/nodejs` SDK.
2. ImageKit returns a permanent CDN URL (e.g. `https://ik.imagekit.io/…`).
3. The CDN URL is stored as metadata in Azure Blob Storage for record-keeping.
4. The user receives only the CDN URL as the "Image Link".

Upload resilience: 60-second timeout with 3 automatic retries and linear backoff.

### Azure Blob Storage

Azure Blob Storage (`oclite-gallery` container) serves as the metadata and backup layer. Each blob stores the original PNG plus metadata headers (prompt, model, user, ImageKit URL). Access is via a long-lived Account SAS token.

> **Metadata safety:** All metadata values are sanitised to ASCII-only (0x20-0x7E) before upload to avoid `Invalid character in header content` errors.

### Secrets Management

All secrets (API keys, SAS URLs, ImageKit credentials) are **XOR-encrypted at rest** inside `src/utilities/secrets.ts` and decrypted at runtime. The `secrets.ts` file is excluded from version control via `.gitignore`. The production bundle (`dist/extension.js`) is verified after each build to ensure no plain-text secrets are present.

### CI/CD Pipeline

The extension uses **GitHub Actions** for continuous integration and deployment:

1. Push to `main` triggers the build pipeline.
2. TypeScript compilation and linting.
3. Automated packaging via `vsce package`.
4. Publishing to the VS Code Marketplace.

## Intelligence Layer — Agentic Reasoning

### Prompt Engineering Orchestration

The extension uses **GPT-4o mini** as its reasoning engine. When a user submits a short description, the LLM analyzes the intent and automatically generates a professional SDXL prompt — determining style, color palette, lighting, composition, and quality tags.

If the LLM is unavailable, the system falls back to local prompt enhancement using category-specific presets.

### Context-Aware Suggestion

The agent scans the workspace using `vscode.workspace.findFiles()` and detects the project type:

| Detection | Project Type | Suggested Style |
| :--- | :--- | :--- |
| `*.unity`, `Assets/**/*.cs` | Unity | Character / Texture |
| `*.uproject` | Unreal Engine | Environment / Texture |
| `project.godot` | Godot | Pixel Art |
| `package.json` with `react` | React | UI Icon |
| `package.json` with `vue` | Vue.js | UI Icon |
| `package.json` with `@angular/core` | Angular | UI Icon |
| `*.py` | Python | Vector |

### Smart Auto-Naming

File names are generated by GPT-4o mini based on prompt content (e.g., `cyberpunk_cityscape_01.png`). If the LLM call fails, the system extracts keywords from the prompt and generates a `snake_case` name with versioning.

## Components

### Extension Entry Point — `src/extension.ts`

- Registers the `@oclite` chat participant with the VS Code Chat API.
- Registers commands: `oclite.setApiKey`, `oclite.saveImage`, `oclite.previewImage`.
- Orchestrates the full generation flow: LLM refinement → image generation → polling → download → save.
- Displays LLM vs. fallback status to the user in chat output.

### AI Service — `src/services/ai.ts`

- **`callLLM()`** — Sends chat completion requests to GPT-4o mini via the SumoPod AI endpoint (OpenAI-compatible). Returns the assistant message content or `null` on failure.
- **`refinePrompt()`** — Constructs a system prompt for SDXL optimization and calls the LLM. Returns `{ prompt, fromLLM }` so callers can distinguish between LLM and fallback results.
- **`generateName()`** — Asks the LLM for a descriptive filename. Falls back to keyword extraction.
- **`getWorkspaceSuggestion()`** — Scans workspace files and returns a project type with a suggested asset style.

### Sidebar Provider — `src/panels/SidebarProvider.ts`

- Implements `WebviewViewProvider` for the sidebar panel.
- Renders a webview using VS Code UI Toolkit components.
- Handles generation, polling with real-time progress, and save-to-workspace.
- Displays project detection banner with style suggestions.

### Utilities — `src/utilities/`

- **`getNonce.ts`** — Cryptographic nonce for Content Security Policy.
- **`getUri.ts`** — Resolves webview-safe URIs for local resources.

## API Endpoints

### Intelligence (SumoPod AI)

| Method | Endpoint | Purpose |
| :--- | :--- | :--- |
| `POST` | `https://ai.sumopod.com/v1/chat/completions` | LLM prompt refinement and smart naming (GPT-4o mini) |

### Image Generation (OCLite API)

| Method | Endpoint | Purpose |
| :--- | :--- | :--- |
| `POST` | `/api/v1/generate` | Start image generation |
| `GET` | `/api/predictions/{id}` | Poll generation status |

All endpoints use `Authorization: Bearer <api-key>` header.

## Data Flow

1. User enters a prompt (via Chat or Sidebar).
2. `AIService.refinePrompt()` sends the prompt to GPT-4o mini for SDXL optimization.
3. The refined prompt is sent to `/api/v1/generate` with the selected model.
4. If status is `processing` or `starting`, the extension polls `/api/predictions/{id}`.
5. On success, the image is downloaded to a temporary directory.
6. User can preview or save. On save, `AIService.generateName()` suggests a filename via the LLM.

## Technical Specification

| Component | Technology | Role |
| :--- | :--- | :--- |
| LLM (Brain) | GPT-4o mini | Prompt refinement & agent reasoning |
| Image Generation | SDXL (via Replicate) | High-fidelity asset generation |
| Backend | Azure Functions (Node.js) | Serverless API handling |
| Security | Azure Key Vault | Secure API key management |
| UI Framework | VS Code UI Toolkit | Native & seamless UX |
| CI/CD | GitHub Actions | Automated build & deployment |
| Language | TypeScript | Extension source code |
| HTTP Client | Axios | API communication |
| Build | TypeScript Compiler (tsc) | Compilation |
